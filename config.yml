logs:
  log_dir: ./logs
  weights_folder: weights
  fixed_batch_predictions: batch_preds

dataset:
  val:
    __class_fullname__: source.datasets.voc.VOCSegmentationDataset
    root: data
    split: seg11valid

    # Load all images and masks as numpy arrays directly in your RAM.
    # So, be carefull because it will take a lot of RAM!
    cache_images: true
  train:
    __class_fullname__: source.datasets.voc.SBDDSegmentationDataset
    root: data
    split: train

    # Load all images and masks as numpy arrays directly in your RAM.
    # So, be carefull because it will take a lot of RAM!
    cache_images: true

model:
  n_classes: 21
  trainable_upsampling: false
  bilinear_upsampling_init: true

training:

  log_fixed_batch: true

  use_clearml: true

  # Enable channel last format:
  # https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html
  # On my V100 16GB FCN32s with VGG16 backbone trains 1.5x faster using
  # channels last.
  channels_last: true

  # Gradient accumulation step size.
  grad_acc_iters: 1

  # Whether to overfit model on single random image.
  # Setting it to true will end up with the 100-epochs training
  # procedure with single random image for training and validation.
  overfit_single_batch: false

  seed: 39
  device: cuda:0
  epochs: 13
  batch_size: 20
  dataloader_num_workers: 6

# As in paper.
optimizer:
  __class_fullname__: torch.optim.SGD
  lr: 0.0032
  weight_decay: 0.0002
  momentum: 0.9

criterion:
  __class_fullname__: torch.nn.CrossEntropyLoss

  # This is needed because difficult or border pixels are marked
  # with 255. Inspired by paper's authors here:
  # https://github.com/shelhamer/fcn.berkeleyvision.org/blob/1305c7378a9f0ab44b2c936f4d60e4687e3d8743/voc-fcn32s/train.prototxt#L527
  ignore_index: 255

augmentations:
  train:
    transform:
      __class_fullname__: Compose
      transforms:
        - __class_fullname__: albumentations.augmentations.geometric.resize.Resize
          height: 500
          width: 500
          always_apply: true

        # To PyTorch tensors.
        - __class_fullname__: Normalize
          always_apply: true
          max_pixel_value: 255.0
          mean:
            - 0.485
            - 0.456
            - 0.406
          std:
            - 0.229
            - 0.224
            - 0.225
        - __class_fullname__: ToTensorV2
          transpose_mask: true
          always_apply: true
      additional_targets: {'image': 'image', 'mask': 'mask'}
  val:
    transform:
      __class_fullname__: Compose
      transforms:
        - __class_fullname__: albumentations.augmentations.geometric.resize.Resize
          height: 500
          width: 500
          always_apply: true

        # To PyTorch tensors.
        - __class_fullname__: Normalize
          always_apply: true
          max_pixel_value: 255.0
          mean:
            - 0.485
            - 0.456
            - 0.406
          std:
            - 0.229
            - 0.224
            - 0.225
        - __class_fullname__: ToTensorV2
          transpose_mask: true
          always_apply: true
      additional_targets: {'image': 'image', 'mask': 'mask'}